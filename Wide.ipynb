{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cde57a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 897 examples [00:00, 4200.53 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 807/807 [00:00<00:00, 23870.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 90/90 [00:00<00:00, 11333.23 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to C:\\\\Users\\\\Gattupalli Saketh\\\\OneDrive\\Desktop\\wide\\\\fim_dataset\n",
      "Train size: 807\n",
      "Test size: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Load JSON file into a Dataset\n",
    "json_path = r\"C:\\\\Users\\\\Gattupalli Saketh\\\\OneDrive\\Desktop\\\\Preprocessed\\\\cuda_dataset.json\"\n",
    "dataset = Dataset.from_json(json_path)\n",
    "\n",
    "# Split into train and test sets\n",
    "dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "\n",
    "# Save to disk\n",
    "output_dir = r\"C:\\\\Users\\\\Gattupalli Saketh\\\\OneDrive\\Desktop\\wide\\\\fim_dataset\"\n",
    "dataset.save_to_disk(output_dir)\n",
    "\n",
    "print(f\"Dataset saved to {output_dir}\")\n",
    "print(f\"Train size: {len(dataset['train'])}\")\n",
    "print(f\"Test size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba75bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 894 valid samples from 897 total samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments,DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Step 4: Set Hugging Face token (if needed)\n",
    "os.environ[\"HF_TOKEN\"] = \"hf_*********************\"  # Uncomment if using Hugging Face token\n",
    "\n",
    "# Step 5: Load and prepare the JSON dataset\n",
    "def load_json_dataset(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)  # Load the entire JSON file\n",
    "        if isinstance(data, list):  # Expecting an array of objects\n",
    "            return data\n",
    "        else:\n",
    "            raise ValueError(\"Expected a JSON array of objects\")\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON file: {e}\")\n",
    "        raise\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        raise\n",
    "\n",
    "def format_gemma_fim(data_point):\n",
    "    \"\"\"Format dataset for FIM using fim_text field, safely.\"\"\"\n",
    "    fim_text = data_point.get(\"fim_text\")\n",
    "    if fim_text is not None and isinstance(fim_text, str) and fim_text.strip() != \"\":\n",
    "        return {\"text\": fim_text}\n",
    "    else:\n",
    "        return None  # Return None if fim_text is missing or empty\n",
    "\n",
    "# Load and format dataset\n",
    "json_file = \"cuda_dataset.json\"  # Update with your actual file path if different\n",
    "raw_data = load_json_dataset(json_file)\n",
    "\n",
    "# Safely format the dataset\n",
    "formatted_data = []\n",
    "for item in raw_data:\n",
    "    formatted_item = format_gemma_fim(item)\n",
    "    if formatted_item is not None:\n",
    "        formatted_data.append(formatted_item)\n",
    "\n",
    "# Now create Dataset\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "# Split dataset into train and test\n",
    "train_test_split = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n",
    "\n",
    "\n",
    "# Configure the Gemma model with 4-bit quantization\n",
    "model_id = \"google/codegemma-2b\"  # Use CodeGemma 2B model for code generation\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Set padding token for tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Prevent warnings\n",
    "\n",
    "#  Configure LoRA for efficient fine-tuning\n",
    "lora_config = LoraConfig(\n",
    "    r=64,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "\n",
    "#  Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gemma-cuda-finetuned\",\n",
    "    per_device_train_batch_size=1,  # Adjust based on GPU memory\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch size\n",
    "    warmup_steps=10,\n",
    "    max_steps=100,  # Adjust based on dataset size\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,  # Mixed precision training\n",
    "    logging_steps=1,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    save_strategy=\"epoch\",\n",
    "    # The `evaluation_strategy` argument is deprecated in newer versions of Transformers.\n",
    "    # Instead, use `eval_steps` to control the evaluation frequency.\n",
    "    # evaluation_strategy=\"steps\", \n",
    "    eval_steps=20,  \n",
    "    push_to_hub=False,  # Set to False to avoid pushing to Hub\n",
    "    report_to=\"none\"  # Disable wandb logging\n",
    ")\n",
    "\n",
    "#  Initialize SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=lora_config,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "#  Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "#  Save the fine-tuned model\n",
    "new_model = \"gemma-cuda-finetuned\"\n",
    "trainer.model.save_pretrained(new_model)\n",
    "tokenizer.save_pretrained(new_model)\n",
    "\n",
    "#  Merge LoRA weights with base model (optional)\n",
    "model = AutoModelForCausalLM.from_pretrained(new_model)\n",
    "model.save_pretrained(f\"{new_model}-merged\")\n",
    "tokenizer.save_pretrained(f\"{new_model}-merged\")\n",
    "\n",
    "#  Test the fine-tuned model\n",
    "def generate_cuda_code_fim(prefix, suffix, max_tokens=500):\n",
    "    device = \"cuda:0\"  # Ensure you're using GPU\n",
    "    formatted_prompt = f\"<fim_prefix>{prefix}<fim_suffix>{suffix}<fim_middle>\"\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract the middle part (between <fim_prefix> and <fim_suffix>)\n",
    "    start = response.find(\"<fim_middle>\") + len(\"<fim_middle>\")\n",
    "    end = response.find(\"<fim_suffix>\")\n",
    "    if start >= len(\"<fim_middle>\") and end != -1:\n",
    "        return response[start:end].strip()\n",
    "    return response  # Return full response if parsing fails\n",
    "\n",
    "# Example FIM test\n",
    "test_prefix = \"\"\"#include <errno.h>\n",
    "#include <cuda.h>\n",
    "#include <stdio.h>\n",
    "#define BLOCKS  1\n",
    "#define THREADS 1\n",
    "__global__ void add(int *a, int *b, int *c);\n",
    "int main(void)\n",
    "{\n",
    "    int a, b, c;\"\"\"\n",
    "test_suffix = \"\"\"Memcpy(d_a, &a, size, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_b, &b, size, cudaMemcpyHostToDevice);\n",
    "    add<<< BLOCKS, THREADS >>>(d_a, d_b, d_c);\n",
    "    cudaMemcpy(&c, d_c, size, cudaMemcpyDeviceToHost);\n",
    "    cudaFree(d_a);\n",
    "    cudaFree(d_b);\n",
    "    cudaFree(d_c);\n",
    "    return(EXIT_SUCCESS);\n",
    "}\n",
    "__global__ void add(int *a, int *b, int *c)\n",
    "{\n",
    "    *c = *a + *b;\n",
    "}\"\"\"\n",
    "generated_middle = generate_cuda_code_fim(test_prefix, test_suffix)\n",
    "print(\"Generated CUDA Code (Middle):\")\n",
    "print(generated_middle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
